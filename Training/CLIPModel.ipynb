{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giyushino/MyOwnDoodleGuesser/blob/main/CLIPModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygVjQSuWAiB0",
        "outputId": "1ee7cc38-7513-4339-8448-ec4f3b2a44b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Modules"
      ],
      "metadata": {
        "id": "zcCWhHaKdAhw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N1ZTwTFC_ekL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Embedding"
      ],
      "metadata": {
        "id": "i5InTLhedFGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self, width, max_seq_length):\n",
        "    super().__init__()\n",
        "\n",
        "    # Creating positional encoding\n",
        "    pe = torch.zeros(max_seq_length, width)\n",
        "\n",
        "    for pos in range(max_seq_length):\n",
        "      for i in range(width):\n",
        "        if i % 2 == 0:\n",
        "          pe[pos][i] = np.sin(pos/(10000 ** (i/width)))\n",
        "        else:\n",
        "          pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/width)))\n",
        "\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Add positional encoding to embeddings\n",
        "    x = x + self.pe\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "EkJemy02_o1e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention"
      ],
      "metadata": {
        "id": "BShVHkuAdIV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, width, head_size):\n",
        "    super().__init__()\n",
        "    self.head_size = head_size\n",
        "\n",
        "    self.query = nn.Linear(width, head_size)\n",
        "    self.key = nn.Linear(width, head_size)\n",
        "    self.value = nn.Linear(width, head_size)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    # Obtaining Queries, Keys, and Values\n",
        "    Q = self.query(x)\n",
        "    K = self.key(x)\n",
        "    V = self.value(x)\n",
        "\n",
        "    # Dot Product of Queries and Keys\n",
        "    attention = Q @ K.transpose(-2,-1)\n",
        "\n",
        "    # Scaling\n",
        "    attention = attention / (self.head_size ** 0.5)\n",
        "\n",
        "    # Applying Attention Mask\n",
        "    if mask is not None:\n",
        "        attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "    attention = torch.softmax(attention, dim=-1)\n",
        "\n",
        "    attention = attention @ V\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "SHKmUq7V_qhH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, width, n_heads):\n",
        "    super().__init__()\n",
        "    self.head_size = width // n_heads\n",
        "\n",
        "    self.W_o = nn.Linear(width, width)\n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(width, self.head_size) for _ in range(n_heads)])\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    # Combine attention heads\n",
        "    out = torch.cat([head(x, mask=mask) for head in self.heads], dim=-1)\n",
        "\n",
        "    out = self.W_o(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "pLdbE5aLG4fH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder"
      ],
      "metadata": {
        "id": "_wYeHJF2dPH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, width, n_heads, r_mlp=4):\n",
        "        super().__init__()\n",
        "        self.width = width\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Sub-Layer 1 Normalization\n",
        "        self.ln1 = nn.LayerNorm(width)\n",
        "\n",
        "        # Multi-Head Attention\n",
        "        self.mha = MultiHeadAttention(width, n_heads)\n",
        "\n",
        "        # Sub-Layer 2 Normalization\n",
        "        self.ln2 = nn.LayerNorm(width)\n",
        "\n",
        "        # Multilayer Perception\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.width, self.width*r_mlp),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.width*r_mlp, self.width)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Residual Connection After Sub-Layer 1\n",
        "        x = x + self.mha(self.ln1(x), mask=mask)\n",
        "\n",
        "        # Residual Connection After Sub-Layer 2\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "RlLXEFKY_sI3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "IxNu9T7fdRlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(text, encode=True, mask=None, max_seq_length=32):\n",
        "    if encode:\n",
        "        out = chr(2) + text + chr(3) # Adding SOT and EOT tokens\n",
        "        out = out + \"\".join([chr(0) for _ in range(max_seq_length-len(out))]) # Adding Padding\n",
        "        out = torch.IntTensor(list(out.encode(\"utf-8\"))) # Encoding Text\n",
        "        mask = torch.ones(len(out.nonzero()))\n",
        "        mask = torch.cat((mask,torch.zeros(max_seq_length-len(mask)))).type(torch.IntTensor)\n",
        "    else:\n",
        "        out = [chr(x) for x in text[1:len(mask.nonzero())-1]]\n",
        "        out = \"\".join(out)\n",
        "        mask = None\n",
        "\n",
        "    return out, mask"
      ],
      "metadata": {
        "id": "Oo4r9vqWdViQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Encoder"
      ],
      "metadata": {
        "id": "wdBJQrIsdVxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, width, max_seq_length, n_heads, n_layers, emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_seq_length = max_seq_length  # Maximum length of input sequence\n",
        "\n",
        "        self.encoder_embedding = nn.Embedding(vocab_size, width) # Embedding Table\n",
        "\n",
        "        self.positional_embedding = PositionalEmbedding(width, max_seq_length)\n",
        "\n",
        "        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])\n",
        "\n",
        "        # learned proj of image to embed\n",
        "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
        "\n",
        "    def forward(self, text, mask=None):\n",
        "        # Text Embedding\n",
        "        x = self.encoder_embedding(text)\n",
        "\n",
        "        # Positional Embedding\n",
        "        x = self.positional_embedding(x)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        for encoder_layer in self.encoder:\n",
        "            x = encoder_layer(x, mask=mask)\n",
        "\n",
        "        # Takes features from the EOT Embedding\n",
        "        x = x[torch.arange(text.shape[0]),torch.sub(torch.sum(mask[:,0],dim=1),1)]\n",
        "\n",
        "        # joint multimodal embedding\n",
        "        if self.projection is not None:\n",
        "            x = x @ self.projection\n",
        "\n",
        "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "sHEAV-Vq5Df5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Encoder"
      ],
      "metadata": {
        "id": "TZzh6RMgdXt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, width, img_size, patch_size, n_channels, n_layers, n_heads, emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
        "        assert width % n_heads == 0, \"width must be divisible by n_heads\"\n",
        "\n",
        "        self.n_patches = (img_size[0] * img_size[1]) // (patch_size[0] * patch_size[1])\n",
        "\n",
        "        self.max_seq_length = self.n_patches + 1\n",
        "\n",
        "        # Patch Embedding\n",
        "        self.linear_project = nn.Conv2d(n_channels, width, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Classification Token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, width))\n",
        "\n",
        "        self.positional_embedding = PositionalEmbedding(width,self.max_seq_length)\n",
        "\n",
        "        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])\n",
        "\n",
        "        # learned proj of image to embed\n",
        "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # Patch Embedding\n",
        "        x = self.linear_project(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # Positional Embedding\n",
        "        x = torch.cat((self.cls_token.expand(x.size()[0], -1, -1),x), dim=1)\n",
        "        x = self.positional_embedding(x)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        for encoder_layer in self.encoder:\n",
        "            x = encoder_layer(x)\n",
        "\n",
        "        # Takes Class Tokens\n",
        "        x = x[:, 0, :]\n",
        "\n",
        "        # joint multimodal embedding\n",
        "        if self.projection is not None:\n",
        "            x = x @ self.projection\n",
        "\n",
        "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "k1WvEoo4_uLi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Model"
      ],
      "metadata": {
        "id": "Y07tBsSNdZmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP(nn.Module):\n",
        "    def __init__(self, emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_encoder = ImageEncoder(vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, emb_dim)\n",
        "\n",
        "        self.text_encoder = TextEncoder(vocab_size, text_width, max_seq_length, text_heads, text_layers, emb_dim)\n",
        "\n",
        "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    def forward(self,image,text, mask=None):\n",
        "        I_e = self.image_encoder(image)\n",
        "        T_e = self.text_encoder(text, mask=mask)\n",
        "\n",
        "        # scaled pairwise cosine similarities [n, n]\n",
        "        logits = (I_e @ T_e.transpose(-2,-1)) * torch.exp(self.temperature)\n",
        "\n",
        "        # symmetric loss function\n",
        "        labels = torch.arange(logits.shape[0]).to(self.device)\n",
        "\n",
        "        loss_i = nn.functional.cross_entropy(logits.transpose(-2,-1), labels)\n",
        "        loss_t = nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "        loss = (loss_i + loss_t) / 2\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "6t-K-tnv_xwk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "61_u4dyVAiHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNIST(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        self.dataset = load_dataset(\"fashion_mnist\")\n",
        "\n",
        "        self.transform = T.ToTensor()\n",
        "\n",
        "        if train:\n",
        "            self.split = \"train\"\n",
        "        else:\n",
        "            self.split = \"test\"\n",
        "\n",
        "\n",
        "        self.captions = {0: \"An image of a t-shirt/top\",\n",
        "                        1: \"An image of trousers\",\n",
        "                        2: \"An image of a pullover\",\n",
        "                        3: \"An image of a dress\",\n",
        "                        4: \"An image of a coat\",\n",
        "                        5: \"An image of a sandal\",\n",
        "                        6: \"An image of a shirt\",\n",
        "                        7: \"An image of a sneaker\",\n",
        "                        8: \"An image of a bag\",\n",
        "                        9: \"An image of an ankle boot\"}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.num_rows[self.split]\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        img = self.dataset[self.split][i][\"image\"]\n",
        "        img = self.transform(img)\n",
        "\n",
        "        cap, mask = tokenizer(self.captions[self.dataset[self.split][i][\"label\"]])\n",
        "\n",
        "        mask = mask.repeat(len(mask),1)\n",
        "\n",
        "        return {\"image\": img, \"caption\": cap, \"mask\": mask}"
      ],
      "metadata": {
        "id": "PFk3DN4c_8-Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameters"
      ],
      "metadata": {
        "id": "XzjYQ0NYAjwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 32\n",
        "vit_width = 9\n",
        "img_size = (28,28)\n",
        "patch_size = (14,14)\n",
        "n_channels = 1\n",
        "vit_layers = 3\n",
        "vit_heads = 3\n",
        "vocab_size = 256\n",
        "text_width = 32\n",
        "max_seq_length = 32\n",
        "text_heads = 8\n",
        "text_layers = 4\n",
        "lr = 1e-3\n",
        "epochs = 10\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "jYGpMX_PNXi6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"fashion_mnist\")\n",
        "\n",
        "print(dataset[\"train\"][1])"
      ],
      "metadata": {
        "id": "myU7B9fhgk_b",
        "outputId": "b7b662a6-a64b-42ae-eed1-391b47216153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7B3B25CE58A0>, 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "dataset2 = []\n",
        "files = [\"full_numpy_bitmap_crab.npy\", \"full_numpy_bitmap_crocodile.npy\", \"full_numpy_bitmap_lion.npy\",\n",
        "         \"full_numpy_bitmap_lobster.npy\", \"full_numpy_bitmap_monkey.npy\", \"full_numpy_bitmap_octopus.npy\",\n",
        "         \"full_numpy_bitmap_panda.npy\", \"full_numpy_bitmap_swan.npy\"]\n",
        "\n",
        "\n",
        "# For each class, we are giving them a corresponding label: 0 to 5\n",
        "class_labels = {\n",
        "    \"full_numpy_bitmap_crab.npy\": 0,\n",
        "    \"full_numpy_bitmap_crocodile.npy\": 1,\n",
        "    \"full_numpy_bitmap_lion.npy\": 2,\n",
        "    \"full_numpy_bitmap_lobster.npy\": 3,\n",
        "    \"full_numpy_bitmap_monkey.npy\": 4,\n",
        "    \"full_numpy_bitmap_octopus.npy\": 5,\n",
        "    \"full_numpy_bitmap_panda.npy\": 6,\n",
        "    \"full_numpy_bitmap_swan.npy\": 7\n",
        "}\n",
        "\n",
        "for filename in files:\n",
        "    images = np.load(filename)\n",
        "    print(f\"Loaded {filename} with shape: {images.shape}\")\n",
        "\n",
        "    t_0 = time.perf_counter()\n",
        "    count = 0\n",
        "\n",
        "    # Loop through each image in the file\n",
        "    for i in range(len(images)):\n",
        "        # Only process the first 1000 images from each class\n",
        "\n",
        "        image = images[i]  # Provides (728,) array\n",
        "        reshape = image.reshape(28, 28)  # Reshapes to (28, 28) numpy array\n",
        "        image = Image.fromarray(reshape)\n",
        "        grayscale_image = image.convert(\"L\")\n",
        "\n",
        "        # Assign the label based on the class of the file\n",
        "        label = class_labels[filename]  # Get the label for the class\n",
        "\n",
        "        data = {\n",
        "            'image': grayscale_image,  # The image tensor\n",
        "            'label': label  # The corresponding label (class)\n",
        "        }\n",
        "\n",
        "        dataset2.append(data)\n",
        "        count += 1\n",
        "\n",
        "    t_1 = time.perf_counter()\n",
        "    print(f\"Successfully processed {filename} in {t_1 - t_0:.2f} seconds\")"
      ],
      "metadata": {
        "id": "WRvD-dvMgGmY",
        "outputId": "4ff3137c-4d64-4d66-865d-03a69f24fa56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded full_numpy_bitmap_crab.npy with shape: (126930, 784)\n",
            "Successfully processed full_numpy_bitmap_crab.npy in 3.00 seconds\n",
            "Loaded full_numpy_bitmap_crocodile.npy with shape: (127932, 784)\n",
            "Successfully processed full_numpy_bitmap_crocodile.npy in 3.47 seconds\n",
            "Loaded full_numpy_bitmap_lion.npy with shape: (120949, 784)\n",
            "Successfully processed full_numpy_bitmap_lion.npy in 2.50 seconds\n",
            "Loaded full_numpy_bitmap_lobster.npy with shape: (140175, 784)\n",
            "Successfully processed full_numpy_bitmap_lobster.npy in 2.87 seconds\n",
            "Loaded full_numpy_bitmap_monkey.npy with shape: (127633, 784)\n",
            "Successfully processed full_numpy_bitmap_monkey.npy in 2.81 seconds\n",
            "Loaded full_numpy_bitmap_octopus.npy with shape: (150152, 784)\n",
            "Successfully processed full_numpy_bitmap_octopus.npy in 3.96 seconds\n",
            "Loaded full_numpy_bitmap_panda.npy with shape: (113613, 784)\n",
            "Successfully processed full_numpy_bitmap_panda.npy in 2.24 seconds\n",
            "Loaded full_numpy_bitmap_swan.npy with shape: (152088, 784)\n",
            "Successfully processed full_numpy_bitmap_swan.npy in 3.34 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2[0]"
      ],
      "metadata": {
        "id": "hdTIcRnLgv_v",
        "outputId": "bb66ac01-42e5-479f-ad1e-e85dcc45b22c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.Image.Image image mode=L size=28x28>, 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"].num_rows)"
      ],
      "metadata": {
        "id": "4EoE4N_di9GZ",
        "outputId": "7c90c2b3-3dd9-48b9-b2ec-cf55e4dda3f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset2))"
      ],
      "metadata": {
        "id": "4zQW5blMjC7I",
        "outputId": "938b2a88-2086-4dd9-dca3-af4591df0026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1059472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = dataset2[0][\"image\"]\n",
        "print(img)"
      ],
      "metadata": {
        "id": "Iv_wsgRjhw3C",
        "outputId": "68374f35-37c9-4da3-d630-c956ddddc0f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=L size=28x28 at 0x7B3B1C54B3A0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(dataset2)"
      ],
      "metadata": {
        "id": "qslRZrTwpS5e"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smaller = dataset2[0:100000]"
      ],
      "metadata": {
        "id": "Dw6pFYbu0SfF"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smaller[100]"
      ],
      "metadata": {
        "id": "WVmxVbt40dBS",
        "outputId": "ed862753-0be6-4009-e8a5-1004ed388253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.Image.Image image mode=L size=28x28>, 'label': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = dataset2[0][\"image\"]\n",
        "to_tensor = T.ToTensor()\n",
        "img = to_tensor(img)\n",
        "\n",
        "print(img)"
      ],
      "metadata": {
        "id": "zipI4UVwhOlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.dataset = smaller\n",
        "\n",
        "        self.transform = T.ToTensor()\n",
        "\n",
        "\n",
        "        self.captions = {0: \"a drawing of a crab\",\n",
        "                        1: \"a drawing of a crocodile\",\n",
        "                        2: \"a drawing of a lion\",\n",
        "                        3: \"a drawing of a lobster\",\n",
        "                        4: \"a drawing of a monkey\",\n",
        "                        5: \"a drawing of a octopus\",\n",
        "                        6: \"a drawing of a panda\",\n",
        "                        7: \"a drawing of a swan\"}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        img = self.dataset[i][\"image\"]\n",
        "        img = self.transform(img)\n",
        "\n",
        "        cap, mask = tokenizer(self.captions[self.dataset[i][\"label\"]])\n",
        "\n",
        "        mask = mask.repeat(len(mask),1)\n",
        "\n",
        "        return {\"image\": img, \"caption\": cap, \"mask\": mask}"
      ],
      "metadata": {
        "id": "t7zr-2MciRV1"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = MyCustomDataset()\n",
        "test_set = MyCustomDataset()\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_set, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "qqIrOLJVfxZO"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_trainset = FashionMNIST()\n",
        "real_testset = FashionMNIST()\n",
        "\n",
        "real_train_loader = DataLoader(real_trainset, shuffle=True, batch_size=batch_size)\n",
        "real_test_loader = DataLoader(real_testset, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "wxoJ2x2PqAEM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 7\n",
        "\n",
        "print(train_set[i][\"image\"].shape)\n",
        "print(train_set[i][\"caption\"].shape)\n",
        "print(train_set[i][\"mask\"].shape)\n",
        "print(train_set[i][\"caption\"])\n",
        "print(train_set[i][\"mask\"])"
      ],
      "metadata": {
        "id": "weW8PJh5jS-3",
        "outputId": "6fc5fa63-8669-47a0-e9a9-01394522deca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 32])\n",
            "tensor([  2,  97,  32, 100, 114,  97, 119, 105, 110, 103,  32, 111, 102,  32,\n",
            "         97,  32, 108, 111,  98, 115, 116, 101, 114,   3,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0], dtype=torch.int32)\n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 11\n",
        "\n",
        "print(real_testset[i][\"image\"].shape)\n",
        "print(real_testset[i][\"caption\"].shape)\n",
        "print(real_testset[i][\"mask\"].shape)\n",
        "print(real_testset[i][\"caption\"])\n",
        "print(real_testset[i][\"mask\"])"
      ],
      "metadata": {
        "id": "Qy6YpB_OqPP4",
        "outputId": "e0b2780a-3b9d-419e-9c04-85171ffbc342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 32])\n",
            "tensor([  2,  65, 110,  32, 105, 109,  97, 103, 101,  32, 111, 102,  32,  97,\n",
            "        110,  32,  97, 110, 107, 108, 101,  32,  98, 111, 111, 116,   3,   0,\n",
            "          0,   0,   0,   0], dtype=torch.int32)\n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 32\n",
        "vit_width = 9\n",
        "img_size = (28,28)\n",
        "patch_size = (14,14)\n",
        "n_channels = 1\n",
        "vit_layers = 3\n",
        "vit_heads = 3\n",
        "vocab_size = 256\n",
        "text_width = 32\n",
        "max_seq_length = 32\n",
        "text_heads = 8\n",
        "text_layers = 4\n",
        "lr = 1e-3\n",
        "epochs = 10\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "kywA5eUkjeBq"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
        "\n",
        "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "best_loss = np.inf\n",
        "for epoch in range(epochs):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        img, cap, mask = data[\"image\"].to(device), data[\"caption\"].to(device), data[\"mask\"].to(device)\n",
        "        loss = model(img,cap,mask)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Saves model if it performed better than the previous best\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.3f}\")\n",
        "    if loss.item() <= best_loss:\n",
        "        best_loss = loss.item()\n",
        "        torch.save(model.state_dict(), \"/content/clip2.pt\")\n",
        "        print(\"Model Saved.\")"
      ],
      "metadata": {
        "id": "-jzEzlvXjeJc",
        "outputId": "a3086637-0541-4d28-f2d2-edf2e8ff2010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cuda (Tesla T4)\n",
            "Epoch [1/10], Batch Loss: 2.757\n",
            "Model Saved.\n",
            "Epoch [2/10], Batch Loss: 2.485\n",
            "Model Saved.\n",
            "Epoch [3/10], Batch Loss: 2.232\n",
            "Model Saved.\n",
            "Epoch [4/10], Batch Loss: 2.543\n",
            "Epoch [5/10], Batch Loss: 2.323\n",
            "Epoch [6/10], Batch Loss: 2.021\n",
            "Model Saved.\n",
            "Epoch [7/10], Batch Loss: 2.537\n",
            "Epoch [8/10], Batch Loss: 2.653\n",
            "Epoch [9/10], Batch Loss: 2.214\n",
            "Epoch [10/10], Batch Loss: 2.370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "YyefO6DmAmFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "f9XApEuyAutN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "hTy6uuNvBGIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Best Model\n",
        "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/clip2.pt\", map_location=device))\n",
        "\n",
        "# Getting dataset captions to compare images to\n",
        "text = torch.stack([tokenizer(x)[0] for x in test_set.captions.values()]).to(device)\n",
        "mask = torch.stack([tokenizer(x)[1] for x in test_set.captions.values()])\n",
        "mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)\n",
        "\n",
        "correct, total = 0,0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data[\"image\"].to(device), data[\"caption\"].to(device)\n",
        "        image_features = model.image_encoder(images)\n",
        "        text_features = model.text_encoder(text, mask=mask)\n",
        "\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        similarity = (100.0 * (image_features @ text_features.T)).softmax(dim=-1)\n",
        "        _, indices = torch.max(similarity,1)\n",
        "        pred = torch.stack([tokenizer(test_set.captions[int(i)])[0] for i in indices]).to(device)\n",
        "        correct += int(sum(torch.sum((pred==labels),dim=1)//len(pred[0])))\n",
        "        total += len(labels)\n",
        "\n",
        "print(f'\\nModel Accuracy: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L28A-eOAFzY",
        "outputId": "04a0d48c-173f-4112-f079-3ee681b6cd69"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-c2946b814a76>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/clip2.pt\", map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Accuracy: 70 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 1000\n",
        "\n",
        "img = test_set[idx][\"image\"]\n",
        "plt.imshow(img[0]  ,cmap=\"gray\")\n",
        "plt.title(tokenizer(test_set[idx][\"caption\"], encode=False, mask=test_set[idx][\"mask\"][0])[0])\n",
        "img = img.to(device)\n",
        "print(img.shape)\n",
        "img = img.unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "  image_features = model.image_encoder(img)\n",
        "  text_features = model.text_encoder(text, mask=mask)\n",
        "\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{class_names[int(index)]:>16s}: {100 * value.item():.2f}%\")"
      ],
      "metadata": {
        "id": "340J6agu5qy3",
        "outputId": "9ebdfe10-608c-4f5d-fc6c-5ad39da6d703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "\n",
            "Top predictions:\n",
            "\n",
            "a drawing of a crab: 99.98%\n",
            "a drawing of a monkey: 0.02%\n",
            "a drawing of a octopus: 0.00%\n",
            "a drawing of a lobster: 0.00%\n",
            "a drawing of a panda: 0.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot Classification"
      ],
      "metadata": {
        "id": "tvCOw3x9BIRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Best Model\n",
        "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/clip2.pt\", map_location=device))\n",
        "\n",
        "\n",
        "# Captions to compare images to\n",
        "class_names = [\"a drawing of a crab\", \"a drawing of a crocodile\", \"a drawing of a lion\", \"a drawing of a lobster\",\n",
        "          \"a drawing of a monkey\", \"a drawing of a octopus\", \"a drawing of a panda\", \"a drawing of a swan\"]\n",
        "\n",
        "# class_names =[\"An image of a t-shirt/top\",\n",
        "#                         \"An image of trousers\",\n",
        "#                         \"An image of a pullover\",\n",
        "#                         \"An image of a dress\",\n",
        "#                         \"An image of a coat\",\n",
        "#                         \"An image of a sandal\",\n",
        "#                         \"An image of a shirt\",\n",
        "#                         \"An image of a sneaker\",\n",
        "#                         \"An image of a bag\",\n",
        "#                         \"An image of an ankle boot\"]\n",
        "\n",
        "text = torch.stack([tokenizer(x)[0] for x in class_names]).to(device)\n",
        "mask = torch.stack([tokenizer(x)[1] for x in class_names])\n",
        "mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)\n",
        "\n",
        "idx = 1000\n",
        "\n",
        "img = test_set[idx][\"image\"]\n",
        "plt.imshow(img[0]  ,cmap=\"gray\")\n",
        "plt.title(tokenizer(test_set[idx][\"caption\"], encode=False, mask=test_set[idx][\"mask\"][0])[0])\n",
        "plt.show()\n",
        "img = img.to(device)\n",
        "with torch.no_grad():\n",
        "  image_features = model.image_encoder(img)\n",
        "  text_features = model.text_encoder(text, mask=mask)\n",
        "\n",
        "\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{class_names[int(index)]:>16s}: {100 * value.item():.2f}%\")"
      ],
      "metadata": {
        "id": "QPdsnGAwAJoF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
